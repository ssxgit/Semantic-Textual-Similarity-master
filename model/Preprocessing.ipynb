{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from Preprocessing import loadfile\n",
    "import nltk\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(word_list):\n",
    "    stop_words = stopwords.words('english')\n",
    "    filter_words = [w for w in word_list if not w in stop_words]\n",
    "    stopWords =  [w for w in word_list if w in stop_words]\n",
    "    stopWords = \" \".join(stopWords) + '\\n'\n",
    "    with open(\"stopwords.txt\", \"a\") as f:\n",
    "        f.write(stopWords)\n",
    "    return filter_words\n",
    "\n",
    "def wordnet(sentence, stopwords = True):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    words = [wordnet_lemmatizer.lemmatize(word, 'n').lower() for word in sentence.split()]\n",
    "    words = [wordnet_lemmatizer.lemmatize(word, 'a').lower() for word in words]\n",
    "    words = [wordnet_lemmatizer.lemmatize(word, 'v').lower() for word in words]\n",
    "    translation_table = str.maketrans(string.punctuation + string.ascii_uppercase + string.digits,\n",
    "                                      \" \" * len(string.punctuation) + string.ascii_lowercase + \" \" * len(string.digits))\n",
    "    if stopwords:\n",
    "        words = remove_stopwords(words)\n",
    "    words = [word for word in words]\n",
    "    new_sent =  ' '.join(words)\n",
    "    new_sent = new_sent.translate(translation_table)\n",
    "    return new_sent\n",
    "\n",
    "def loadfile(filename):\n",
    "    new_lines = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        content = f.read()\n",
    "        lines = content.split(\"\\n\")\n",
    "        for line in lines:\n",
    "            new_lines.append(line.split(\"\\t\"))\n",
    "    return new_lines\n",
    "\n",
    "def lines_processing(lines, stopwords = True):\n",
    "    new_content = \"\"\n",
    "    line_len = len(lines[0]) > 3\n",
    "    for line in lines:\n",
    "        sen1 = wordnet(line[1], stopwords)\n",
    "        sen2 = wordnet(line[2], stopwords)\n",
    "        if line_len :\n",
    "            line = line[0] + \"\\t\" + sen1 + \"\\t\" + sen2 + \"\\t\" + line[3] +\"\\n\"\n",
    "            print(line)\n",
    "\n",
    "        else:\n",
    "            line = line[0] + \"\\t\" + sen1 + \"\\t\" + sen2 + \"\\n\"\n",
    "        new_content += line\n",
    "    if line_len:\n",
    "        with open(\"train.txt\",\"w\") as f:\n",
    "            f.write(new_content)\n",
    "    else:\n",
    "        with open(\"test.txt\",\"w\") as f:\n",
    "            f.write(new_content)\n",
    "\n",
    "lines = loadfile(\"test_ai-lab.txt\")\n",
    "\n",
    "lines_processing(lines = lines,stopwords = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f4b168067dc7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m# nert_tagger = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ],
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.corpus.reader import WordNetError\n",
    "from sklearn.metrics import r2_score, make_scorer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import brown, wordnet\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from matplotlib import pyplot as plt\n",
    "from Preprocessing import loadfile\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "# nert_tagger = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz')\n",
    "# pos_tagger = StanfordPOSTagger('english-bidirectional-distsim.tagger')\n",
    "# parser = StanfordParser('edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz')\n",
    "\n",
    "def vertorlize(content):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit(content)\n",
    "    return X.toarray()\n",
    "\n",
    "\n",
    "def bag_of_words(sen1,sen2):\n",
    "    return  cosine_similarity([sen1],[sen2])[0][0]\n",
    "\n",
    "def topic_id(all_sens):\n",
    "    lda = LatentDirichletAllocation(n_topics=6,\n",
    "                                    learning_offset=50.,\n",
    "                                    random_state=0)\n",
    "    docres = lda.fit_transform(all_sens)\n",
    "    return docres\n",
    "\n",
    "def tf_idf(sen1,sen2):\n",
    "#     print(\"s1:\",sen1,\"s2:\",sen2)\n",
    "    transformer = TfidfTransformer()\n",
    "    tf_idf = transformer.fit_transform([sen1,sen2]).toarray()\n",
    "    return tf_idf[0], tf_idf[1]\n",
    "\n",
    "\n",
    "def lcs_dp(input_x, input_y):\n",
    "    # input_y as column, input_x as row\n",
    "    dp = [([0] * len(input_y)) for i in range(len(input_x))]\n",
    "    maxlen = maxindex = 0\n",
    "    for i in range(0, len(input_x)):\n",
    "        for j in range(0, len(input_y)):\n",
    "            if input_x[i] == input_y[j]:\n",
    "                if i != 0 and j != 0:\n",
    "                    dp[i][j] = dp[i - 1][j - 1] + 1\n",
    "                if i == 0 or j == 0:\n",
    "                    dp[i][j] = 1\n",
    "                if dp[i][j] > maxlen:\n",
    "                    maxlen = dp[i][j]\n",
    "                    maxindex = i + 1 - maxlen\n",
    "                    # print('最长公共子串的长度是:%s' % maxlen)\n",
    "                    # print('最长公共子串是:%s' % input_x[maxindex:maxindex + maxlen])\n",
    "    return maxlen,input_x[maxindex:maxindex + maxlen]\n",
    "\n",
    "def not_empty(s):\n",
    "    return s and s.strip()\n",
    "\n",
    "\n",
    "def tag_and_parser(str_sen1,str_sen2):\n",
    "#     print(str_sen1.split(\" \"),str_sen2)\n",
    "    sen1 = list(filter(not_empty, str_sen1.split(\" \")))\n",
    "    sen2 = list(filter(not_empty, str_sen2.split(\" \")))\n",
    "    # print(\"sen1:\",sen1)\n",
    "    post_sen1 = nltk.pos_tag(sen1)\n",
    "    post_sen2 = nltk.pos_tag(sen2)\n",
    "    pos1 ,pos2 = \"\", \"\"\n",
    "    # print(post_sen1,post_sen2)\n",
    "    for word,pos in post_sen1:\n",
    "        pos1 += pos+\" \"\n",
    "    for word,pos in post_sen2:\n",
    "        pos2 += pos+\" \"\n",
    "    # print(pos1,pos2)\n",
    "    maxlen, subseq = lcs_dp(pos1,pos2)\n",
    "    return  len(subseq.split(\" \"))/len(str_sen1.split(' ')), len(subseq.split(\" \"))/len(str_sen2.split(' '))\n",
    "\n",
    "\n",
    "def all_features(content):\n",
    "    vectorlize = CountVectorizer()\n",
    "    sents = []\n",
    "    sents1 = []\n",
    "    sents2 = []\n",
    "    scores = []\n",
    "    for line in content[:-1]:\n",
    "    #         print(line[1])\n",
    "        sents1.append(line[1])\n",
    "        sents2.append(line[2])\n",
    "        sents.append(line[1])\n",
    "        sents.append(line[2])\n",
    "        if len(line) > 3:\n",
    "            scores.append(float(line[3]))\n",
    "\n",
    "    Sents = vectorlize.fit_transform(sents).toarray()\n",
    "    Sents1 = vectorlize.transform(sents1).toarray()\n",
    "    Sents2 = vectorlize.transform(sents2).toarray()\n",
    "    with open(\"model.pickle\",\"wb\") as f:\n",
    "        pickle.dump(vectorlize, f)\n",
    "    tfidf_Sents1 = []\n",
    "    tfidf_Sents2 = []\n",
    "    tfidf_Sents = []\n",
    "    cosine = []\n",
    "    pos_lcs = []\n",
    "    for i in range(len(sents1)):\n",
    "        tfidf_Sent1, tfidf_Sent2 = tf_idf(Sents1[i],Sents2[i])\n",
    "        tfidf_Sents1.append(tfidf_Sent1)\n",
    "        tfidf_Sents2.append(tfidf_Sent2)\n",
    "        tfidf_Sents.append(tfidf_Sent1)\n",
    "        tfidf_Sents.append(tfidf_Sent2)\n",
    "        cosine.append(cosine_similarity([tfidf_Sent1],[tfidf_Sent2])[0][0])\n",
    "        lcs1, lcs2 = tag_and_parser(sents1[i],sents2[i])\n",
    "        pos_lcs.append([lcs1, lcs2])\n",
    "    tp_Sents = topic_id(tfidf_Sents)\n",
    "    return cosine, pos_lcs,tfidf_Sents1,tfidf_Sents2,tp_Sents,scores\n",
    "\n",
    "\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "tagger = nltk.tag.pos_tag\n",
    "frequency_list = FreqDist(i.lower() for i in brown.words())\n",
    "all_words_count = 0\n",
    "for i in frequency_list:\n",
    "    all_words_count += frequency_list[i]\n",
    "\n",
    "\n",
    "def get_words(sentence):\n",
    "    return [i.strip('., ') for i in sentence.split(' ')]\n",
    "\n",
    "\n",
    "with open('word_to_vec.txt', 'r') as f:\n",
    "    embeddings = {}\n",
    "    for line in f.readlines():\n",
    "        args = get_words(line.strip(\"\\n\\t \"))\n",
    "        embeddings[args[0]] = [float(i) for i in args[1:]]\n",
    "\n",
    "def get_ngram(word, n):\n",
    "    ngrams = []\n",
    "    word_len = len(word)\n",
    "    for i in range(word_len - n + 1):\n",
    "        ngrams.append(word[i: i + n])\n",
    "    return ngrams\n",
    "\n",
    "\n",
    "def get_lists_intersection(s1, s2):\n",
    "    s1_s2 = []\n",
    "    for i in s1:\n",
    "        if i in s2:\n",
    "            s1_s2.append(i)\n",
    "    return s1_s2\n",
    "\n",
    "\n",
    "def overlap(sentence1_ngrams, sentence2_ngrams):\n",
    "    s1_len = len(sentence1_ngrams)\n",
    "    s2_len = len(sentence2_ngrams)\n",
    "    if s1_len == 0 and s2_len == 0:\n",
    "        return 0\n",
    "    s1_s2_len = max(1, len(get_lists_intersection(sentence2_ngrams, sentence1_ngrams)))\n",
    "    return 2 / (s1_len / s1_s2_len + s2_len / s1_s2_len)\n",
    "\n",
    "\n",
    "def get_ngram_feature(sentence1, sentence2, n):\n",
    "    sentence1_ngrams = []\n",
    "    sentence2_ngrams = []\n",
    "\n",
    "    for word in sentence1:\n",
    "        sentence1_ngrams.extend(get_ngram(word, n))\n",
    "\n",
    "    for word in sentence2:\n",
    "        sentence2_ngrams.extend(get_ngram(word, n))\n",
    "\n",
    "    return overlap(sentence1_ngrams, sentence2_ngrams)\n",
    "def is_subset(s1, s2):\n",
    "    for i in s1:\n",
    "        if i not in s2:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_numbers_feature(sentence1, sentence2):\n",
    "    s1_numbers = [float(i) for i in re.findall(r\"[-+]?\\d+\\.?\\d*\", \" \".join(sentence1))]\n",
    "    s2_numbers = [float(i) for i in re.findall(r\"[-+]?\\d+\\.?\\d*\", \" \".join(sentence2))]\n",
    "    s1_s2_numbers = []\n",
    "    for i in s1_numbers:\n",
    "        if i in s2_numbers:\n",
    "            s1_s2_numbers.append(i)\n",
    "\n",
    "    s1ands2 = max(len(s1_numbers) + len(s2_numbers), 1)\n",
    "    return [np.log(1 + s1ands2), 2 * len(s1_s2_numbers) / s1ands2,\n",
    "            is_subset(s1_numbers, s2_numbers) or is_subset(s2_numbers, s1_numbers)]\n",
    "\n",
    "\n",
    "def get_shallow_features(sentence):\n",
    "    counter = 0\n",
    "    for word in sentence:\n",
    "        if len(word) > 1 and (re.match(\"[A-Z].*]\", word) or re.match(\"\\.[A-Z]+]\", word)):\n",
    "            counter += 1\n",
    "    return counter\n",
    "\n",
    "\n",
    "def get_word_embedding(inf_content, word):\n",
    "    if inf_content:\n",
    "        return np.multiply(information_content(word), embeddings.get(word, np.zeros(300)))\n",
    "    else:\n",
    "        return embeddings.get(word, np.zeros(300))\n",
    "\n",
    "\n",
    "def sum_embeddings(words, inf_content):\n",
    "    vec = get_word_embedding(inf_content, words[0])\n",
    "    for word in words[1:]:\n",
    "        vec = np.add(vec, get_word_embedding(inf_content, word))\n",
    "    return vec\n",
    "\n",
    "\n",
    "def word_embeddings_feature(sentence1, sentence2):\n",
    "    return cosine_similarity(unpack(sum_embeddings(sentence1, False)),\n",
    "                             unpack(sum_embeddings(sentence2, False)))[0][0]\n",
    "\n",
    "\n",
    "def information_content(word):\n",
    "    return np.log(all_words_count / max(1, frequency_list[word]))\n",
    "\n",
    "\n",
    "def unpack(param):\n",
    "    return param.reshape(1, -1)\n",
    "\n",
    "\n",
    "def weighted_word_embeddings_feature(sentence1, sentence2):\n",
    "    return cosine_similarity(unpack(sum_embeddings(sentence1, True)),\n",
    "                             unpack(sum_embeddings(sentence2, True)))[0][0]\n",
    "\n",
    "\n",
    "def weighted_word_coverage(s1, s2):\n",
    "    s1_s2 = get_lists_intersection(s1, s2)\n",
    "    return np.sum([information_content(i) for i in s1_s2]) / np.sum([information_content(i) for i in s2])\n",
    "\n",
    "\n",
    "def harmonic_mean(s1, s2):\n",
    "    if s1 == 0 or s2 == 0:\n",
    "        return 0\n",
    "    return s1 * s2 / (s1 + s2)\n",
    "\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('A') or treebank_tag.startswith('JJ'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "\n",
    "def get_synset(word):\n",
    "    try:\n",
    "        return wordnet.synset(word + \".\" + get_wordnet_pos(tagger([word])[0][1]) + \".01\")\n",
    "    except :\n",
    "        return 0\n",
    "\n",
    "\n",
    "def wordnet_score(word, s2):\n",
    "    if word in s2:\n",
    "        return 1\n",
    "    else:\n",
    "        similarities = []\n",
    "        for w in s2:\n",
    "            try:\n",
    "                value = get_synset(word).path_similarity(get_synset(w))\n",
    "                if value is None:\n",
    "                    value = 0\n",
    "                similarities.append(value)\n",
    "            except AttributeError:\n",
    "                similarities.append(0)\n",
    "        return np.max(similarities)\n",
    "\n",
    "\n",
    "def wordnet_overlap(s1, s2):\n",
    "    suma = 0\n",
    "    for w in s1:\n",
    "        suma += wordnet_score(w, s2)\n",
    "    return suma / len(s2)\n",
    "\n",
    "\n",
    "def feature_vector(a, b):\n",
    "    fvec = []\n",
    "    # Ngram overlap\n",
    "\n",
    "    fvec.append(get_ngram_feature(a, b, 1))\n",
    "    fvec.append(get_ngram_feature(a, b, 2))\n",
    "    fvec.append(get_ngram_feature(a, b, 3))\n",
    "\n",
    "    # WordNet-aug. overlap -\n",
    "    fvec.append(harmonic_mean(wordnet_overlap(a, b), wordnet_overlap(b, a)))\n",
    "\n",
    "    # Weighted word overlap -\n",
    "    fvec.append(harmonic_mean(weighted_word_coverage(a, b),\n",
    "                              weighted_word_coverage(b, a)))\n",
    "    # sentence num_of_words differences -\n",
    "    fvec.append(abs(len(a) - len(b)))\n",
    "\n",
    "    # summed word embeddings - lagano\n",
    "    fvec.append(word_embeddings_feature(a, b))\n",
    "    fvec.append(weighted_word_embeddings_feature(a, b))\n",
    "\n",
    "    # Shallow NERC - lagano\n",
    "    fvec.append(get_shallow_features(a))\n",
    "    fvec.append(get_shallow_features(b))\n",
    "\n",
    "    # Numbers overlap - returns list of 3 features\n",
    "    fvec.extend(get_numbers_feature(a, b))\n",
    "    return fvec\n",
    "\n",
    "def extract():\n",
    "    content1 = loadfile('train.txt')\n",
    "    cosine, pos_lcs, tfidf_Sents1, tfidf_Sents2, tp_Sents, scores = all_features(content1)\n",
    "    tp_Sents = np.array(tp_Sents)\n",
    "    cosine = np.array(cosine)\n",
    "    pos_lcs = np.array(pos_lcs)\n",
    "    tp_Sents = np.array(tp_Sents)\n",
    "    X_train = np.c_[cosine,pos_lcs,tp_Sents[::2],tp_Sents[1::2]]\n",
    "    Y_train = np.array(scores)\n",
    "\n",
    "    content2 = loadfile(\"test.txt\")\n",
    "    Y_ids = []\n",
    "    for line in content2:\n",
    "        Y_ids.append(line[0])\n",
    "    cosine, pos_lcs, tfidf_Sents1, tfidf_Sents2, tp_Sents, scores = all_features(content2)\n",
    "    tp_Sents = np.array(tp_Sents)\n",
    "    cosine = np.array(cosine)\n",
    "    pos_lcs = np.array(pos_lcs)\n",
    "    tp_Sents = np.array(tp_Sents)\n",
    "    X_test = np.c_[cosine, pos_lcs, tp_Sents[::2], tp_Sents[1::2]]\n",
    "    return X_train, Y_train, X_test, Y_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = loadfile(\"train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a =feature_vector(lines[0][1],lines[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b=feature_vector(lines[1][1],lines[1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c=feature_vector(lines[2][1],lines[2][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d=[]\n",
    "d.append(a)\n",
    "d.append(b)\n",
    "d.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "new_train = scaler.fit_transform([a,b,c])\n",
    "new_train2 = scaler.fit_transform(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 13)\n",
      "[[-0.9350514   0.          0.          1.15456139  1.39285644 -0.16222142\n",
      "   0.          0.          0.          0.          0.          0.          0.        ]\n",
      " [-0.45131006  0.          0.          0.12999648 -0.90847349 -1.13554995\n",
      "   0.          0.          0.          0.          0.          0.          0.        ]\n",
      " [ 1.38636146  0.          0.         -1.28455787 -0.48438296  1.29777137\n",
      "   0.          0.          0.          0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(new_train.shape)\n",
    "print(new_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def F_vec(contents):\n",
    "    train_vec = []\n",
    "    for line in contents:\n",
    "        if len(line) <2:\n",
    "            break\n",
    "        train_vec.append(np.array(feature_vector(line[1], line[2]), dtype=np.float64))\n",
    "    return train_vec\n",
    "\n",
    "\n",
    "def extract():\n",
    "    scaler = StandardScaler()\n",
    "    content1 = loadfile('train.txt')\n",
    "    train_vec = F_vec(content1)\n",
    "    cosine, pos_lcs, tfidf_Sents1, tfidf_Sents2, tp_Sents, scores = all_features(content1)\n",
    "    tp_Sents = np.array(tp_Sents)\n",
    "    cosine = np.array(cosine)\n",
    "    pos_lcs = np.array(pos_lcs)\n",
    "    tp_Sents = np.array(tp_Sents)\n",
    "    X_train = np.c_[cosine,pos_lcs,tp_Sents[::2],tp_Sents[1::2],train_vec]\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    Y_train = np.array(scores)\n",
    "\n",
    "    content2 = loadfile(\"test.txt\")\n",
    "    test_vec = F_vec(content2)\n",
    "    Y_ids = []\n",
    "    for line in content2:\n",
    "        Y_ids.append(line[0])\n",
    "    cosine, pos_lcs, tfidf_Sents1, tfidf_Sents2, tp_Sents, scores = all_features(content2)\n",
    "    tp_Sents = np.array(tp_Sents)\n",
    "    cosine = np.array(cosine)\n",
    "    pos_lcs = np.array(pos_lcs)\n",
    "    tp_Sents = np.array(tp_Sents)\n",
    "    X_test = np.c_[cosine, pos_lcs, tp_Sents[::2], tp_Sents[1::2],test_vec]\n",
    "    X_test = scaler.transform(X_test)\n",
    "    return X_train, Y_train, X_test, Y_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-ba601d26fc04>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-40-f46eea1738c6>\u001b[0m in \u001b[0;36mextract\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mcontent1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloadfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mtrain_vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mcosine\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_lcs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfidf_Sents1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfidf_Sents2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtp_Sents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mtp_Sents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtp_Sents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-40-f46eea1738c6>\u001b[0m in \u001b[0;36mF_vec\u001b[1;34m(contents)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mtrain_vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcontents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mtrain_vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtrain_vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_ids = extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "def model_random_forest(Xtrain,Xtest,y_train):\n",
    "    X_train = Xtrain\n",
    "    rfr = RandomForestRegressor(n_jobs=3, random_state=10,oob_score=True)\n",
    "    param_grid = {'n_estimators': [450]}\n",
    "    # 'n_estimators': list(range(30,601,30)), ,\"max_depth\":list(range(40,61,10)),\"max_features\":list(range(120,211,30)),min_samples_split':list(range(20,201,20)),  'min_samples_leaf':list(range(10,80,10))\n",
    "    model = GridSearchCV(estimator=rfr, param_grid=param_grid, scoring= None, iid=False, cv=10)\n",
    "    model.fit(X_train, y_train)\n",
    "    # model.grid_scores_, model.best_params_, model.best_score_\n",
    "    print('Random forecast classifier...')\n",
    "    print('Best Params:')\n",
    "    print(model.best_params_)\n",
    "    print('Best CV Score:')\n",
    "    print(-model.best_score_)\n",
    "    y_pred = model.predict(Xtest)\n",
    "\n",
    "    return y_pred, -model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forecast classifier...\n",
      "Best Params:\n",
      "{'n_estimators': 450}\n",
      "Best CV Score:\n",
      "-0.5960101886055721\n"
     ]
    }
   ],
   "source": [
    "y_pred, _ = model_random_forest(X_train, X_test, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}